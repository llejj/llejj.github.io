<!DOCTYPE html>
<html>

<meta http-equiv='cache-control' content='no-cache'> 
<meta http-equiv='expires' content='0'> 
<meta http-equiv='pragma' content='no-cache'>

<head>
    <title>Alex Sun's Portfolio</title>
    <link href='./stylesheet.css' rel='stylesheet'>
</head>

<!-- The sidebar 
<div class="sidebar">
    <a class="active" href="#home">Home</a>
    <a href="#stocks">HMM Stocks</a>
    <a href="#speech">HMM Speech Recognition</a>
</div> -->

<body>
    <h1>Alex Sun's Portfolio</h1>
    <p>Hi, I'm an undergraduate at UC Berkeley studying Applied Math and Computer Science graduating in Spring 2025. Below are projects from my <a href="https://github.com/llejj/llejj.github.io/blob/main/resume.pdf">resume</a> that could use elaborating. I hope that, under the guidance of an internship, I can work on larger projects and gain experience solving real-world problems. </p>

    <!--
<h3>ChatGPT API for Translation</h3>
This is a simple Python script that uses ChatGPT to translate text. It takes the original text file and outputs a translated version. I've used it on some previously untranslated novels<br><br>

[insert sample]
-->
    <br>
    <div>
        <center><h3 id="stocks"><a href="https://github.com/llejj/HMM_stocks">HMM Stocks</a></h3></center>
        <p>I used a Hidden Markov Model to analyze historical $SPY data. My model had two hidden states, and its emissions were the daily % price changes.</p>

        <p>The transition matrix:</p>
        <center>
            <table border="1">
                <thead>
                    <tr>
                        <th></th>
                        <th>State 0</th>
                        <th>State 1</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <th>State 0</th>
                        <th>0.975</th>
                        <th>0.025</th>
                    </tr>
                    <tr>
                        <th>State 1</th>
                        <th>0.017</th>
                        <th>0.983</th>
                    </tr>
                </tbody>
            </table>
        </center>

        <p>Probability distribution for each state:</p>
        <center>
            <img src="images/State0.png" alt="State 0's Probability Distribution" class="inline" />
            <img src="images/State1.png" alt="State 1's Probability Distribution" class="inline" />
        </center>

        <p>Most likely sequence of states decoded with the Viterbi Algorithm:</p>
        <center>
            <img src="images/SPY.png" alt="$SPY price and HMM states" class="inline" />
        </center>

        <p>It seems that State 0 reflects more stable bull markets, while State 1 reflects increased volatility during bear
        markets. This model offers a simple description of the stock market.</p>

    </div>

    <br>
    <div>
        <center><h3 id="speech"><a href="https://github.com/llejj/llejj.github.io/blob/main/images/MAT167_Final_Project-1.pdf">PageRank</a></h3></center>
        <object data="images/MAT167_Final_Project-1.pdf" type="application/pdf" width="100%" height="90%">
          <p>Google’s PageRank algorithm models surfing the Web as a random walk. The basic idea is to represent this random walk as a Markov Chain. The PageRank of each website is its probability in the stationary distribution. A website’s PageRank can be interpreted as a measure of its popularity or importance. The PageRank algorithm can be applied to any directed graph, not just to websites. </p>
          <p>In this project, we used PageRank to analyze the LastFM Asia Social Network, where the vertices in the directed graph are users, and the edges are follower relationships between them. We calculated the PageRank by solving a sparse linear system. We found that a small amount of users have much higher PageRank than the typical user. <a href="images/MAT167_Final_Project-1.pdf">Download</a> instead.
          </p>
        </object>
    </div>
    
    <!--
    <br>
    <div>
        <center><h3 id="speech"><a href="https://github.com/llejj/Speech_Recognition">HMM Speech Recognition</a></h3></center>
        <p>I implemented a Hierarchical Hidden Markov Model (HHMM) based on "The Hierarchical Hidden Markov Model: Analysis and Applications" by Shai Fine, Yoram Singer, and Naftali Tishby. In particular, I implemented the Baum-Welch and Viterbi algorithms.</p>

        <p>Then, I began to train an HHMM for speech recognition. The basic idea was: 
            <ol>
                <li>Extract MFCC features from audio signal (which should even out differences in voice)</li>
                <li>The deepest layer of the HHMM decodes phonemes from MFCC features</li>
                <li>The next layer of the HHMM decodes words</li>
                <li>The top layer decodes common word sequences</li>
            </ol>
        </p>

        <p>I trained the deepest layer separately as simple HMMs, but had issues running the Generalized Baum-Welch on the full model. In the future, I plan to rewrite my HHMM more efficiently so that the training process runs smoothly. I worked on this project before I took important classes such as D.S. and Algorithms, so I feel I could do a much better job now.</p>
    </div> -->

</body>

</html>
